### PUT

Параметры запуска:
<ol>
<li>4 потока</li>
<li>64 открытых соединения</li>
<li>2 минуты работы</li>
<li>5000 запросов в секунду</li>
</ol>

```
/async-profiler -d 15 -f cpu-put.svg -e cpu 291344
```

![CPU PUT](/async/cpu-put.svg)
На поток-обработчик теперь тратится сравнительно больше ресурсов CPU, нежели чем на пул рабочих потоков. Было 78%/22%; стало 70%/27%.

Рабочие потоки теперь также тратят ресурсы CPU на передачу запроса на другой узел (метод passOnInternal), который ответственен за исполнение операции и передачу клиенту ответа. А именно, 19% всех ресурсов CPU тратит на данное действие. При этом передача клиенту ответа занимает сравнительно меньшее количество ресурсов, нежели передача делегированному узлу и получение от него ответа.

```
/async-profiler -d 15 -f alloc-put.svg -e alloc 291344
```

![ALLOC PUT](/async/alloc-put.svg)
Видно, что на новой диаграмме появились траты на ресурсы памяти из-за наличия операции passOnInternal, которая выполняет делегирование обработки запроса иному узлу. Большую часть расходов при этой операции берёт на себя метод invoke() класса HttpClient, скорей всего, для хранения результата ответа в виде массива байт. Остальная часть диаграммы изменилась незначительно.

```
/async-profiler -d 15 -f lock-put.svg -e lock 291344
```

![LOCK PUT](/async/lock-put.svg)
При сравнении диаграмм, ясно видно, что теперь ожидания на блокировках в потоке, отвечающем за обработку HTTP-запросов, значительно дольше. На рабочие потоки ожидание приходилось 59%, на поток-обработчик - 41%. Теперь же на рабочие - 44%, на поток-обработчик - 56%. В новом коде методы put, get и delete объединены в метод handleEntityRequest. Так как данный результат профилирования был направлен на выполнение операции put, можем считать метод put старого кода равнозначным методу handleEntityRequest нового кода. Ввиду вышесказанного заметим, что в новой диаграмме появилось дополнительное ожидание при выполнении handleEntityRequest, связанное с тем, что при делегировании обработки HTTP-запроса иному узлу, необходимо делегировать передачу запроса другому узлу рабочему потоку, но все потоки могут быть в данный момент уже заняты, поэтому при выполнении execute() класса ThreadPoolExecutor может происходить ожидание. На диаграмме предыдущего этапа такого нет. Тем не менее при выполнении операции put/handleEntityRequest на обоих диаграммах видим подобное поведение, как результат занятости рабочих потоков.

Когда рабочие потоки не заняты, они ожидают задачи, что видно на левых частях диаграммы. Метод take класса ArrayBlockingQueue ожидает поступления задачи рабочему потоку от потока SelectorThread. В этом нюансе обе диаграммы имеют близкие структуры.

```
wrk2 -t4 -c64 -d2m -R5000 -s wrk/put.lua --latency http://127.0.0.1:8080
Running 2m test @ http://127.0.0.1:8080
  4 threads and 64 connections
  Thread calibration: mean lat.: 1.351ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.352ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.357ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.337ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.21ms  633.99us  23.90ms   78.84%
    Req/Sec     1.32k   117.66     3.11k    76.61%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.15ms
 75.000%    1.54ms
 90.000%    1.88ms
 99.000%    2.57ms
 99.900%    7.20ms
 99.990%   16.99ms
 99.999%   20.53ms
100.000%   23.92ms
 
#[Mean    =        1.215, StdDeviation   =        0.634]
#[Max     =       23.904, Total count    =       549597]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  599836 requests in 2.00m, 52.06MB read
Requests/sec:   4998.68
Transfer/sec:    444.22KB
```
Итоги:
<ol>
<li>обработано 599836 запроса за 2 минуты</li>
<li>прочитано 52.06MB данных</li>
<li>сервер держит заданную нагрузку на уровне 4998.68 запросов в секунду</li>
</ol>

Видно, что скорость обработки запросов по сравнению с предыдущей реализацией (без шардирования) уменьшилась в 2 раза (9997.98/4998.68 = 2), что связано с накладными расходами на делегирование некоторых запросов ответственным за их исполнение узлам.

### GET 

Параметры запуска:
<ol>
<li>4 потока</li>
<li>64 открытых соединения</li>
<li>2 минуты работы</li>
<li>5000 запросов в секунду</li>
</ol>

```
/async-profiler -d 15 -f cpu-get.svg -e cpu 291344 
```

![CPU GET](/async/cpu-get.svg)
Подобно результату анализа диаграмм для cpu-put, здесь мы наблюдаем дополнительные траты процессорных ресурсов на делегирование выполнения операции другому узлу.

Заметим, что вместо метода putInternal, здесь на обоих диаграммах мы видим метод getInternal.

```
/async-profiler -d 15 -f alloc-get.svg -e alloc 291344
```

![ALLOC GET](/async/alloc-get.svg)
Подобно результату анализа диаграмм для alloc-put, здесь мы наблюдаем дополнительные затраты на память при исполнении метода passOnInternal(), отвечающего за делегирование обработки запроса другим узлом. Также, заметим, что поток, обрабатывающий запросы (SelectorThread), тратит дополнительные ресурсы памяти при делегировании обработки операции passOn рабочему потоку из пула.

```
/async-profiler -d 15 -f lock-get.svg -e lock 291344
```

![LOCK GET](/async/lock-get.svg)
По аналогии с результатами анализа сравнения диаграмм при профилировании lock-put, видим, что дополнительные ожидания появились ввиду наличия делегирования ответственному за выполнение операции узлу.

``` 
wrk2 -t4 -c64 -d2m -R5000 -s wrk/get.lua --latency http://127.0.0.1:8080
Running 2m test @ http://127.0.0.1:8080
  4 threads and 64 connections
  Thread calibration: mean lat.: 1.165ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.167ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.152ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.160ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.20ms  689.97us  23.82ms   83.71%
    Req/Sec     1.32k   130.19     3.78k    86.60%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.14ms
 75.000%    1.51ms
 90.000%    1.85ms
 99.000%    2.74ms
 99.900%    9.12ms
 99.990%   16.82ms
 99.999%   21.17ms
100.000%   23.84ms
 
#[Mean    =        1.204, StdDeviation   =        0.690]
#[Max     =       23.824, Total count    =       549598]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  599835 requests in 2.00m, 52.20MB read
Requests/sec:   4998.66
Transfer/sec:    445.48KB

```
Итоги:
<ol>
<li>обработано 599835 запроса за 2 минуты</li>
<li>прочитано 52.20MB данных</li>
<li>сервер держит заданную нагрузку на уровне 4998.66 запросов в секунду</li>
</ol>

Как видно, последствия шардирования таким же образом повлияли при использовании метода get, как и при put, т.е. скорость исполнения запросов понизилась в 2 раза, что можно объяснить тем, что запросы get также делегируются ответственным за исполнение узлам.
